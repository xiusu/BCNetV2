model:
    #channels: [32, 32, 16, 30, 30, 24, 55, 55, 24, 58, 58, 32, 98, 98, 32, 120, 120, 32, 58, 58, 64, 236, 236, 64, 238, 238, 64, 255, 255, 64, 156, 156, 96, 288, 288, 96, 243, 243, 96, 190, 190, 160, 490, 490, 160, 418, 418, 160, 341, 341, 320, 952]
    channels: [32, 32, 16, 96, 96, 24, 144, 144, 24, 144, 144, 32, 192,
               192, 32, 192, 192, 32, 192, 192, 64, 384, 384, 64, 384,
               384, 64, 384, 384, 64, 384, 384, 96, 576, 576, 96, 576,
               576, 96, 576, 576, 160, 960, 960, 160, 960, 960, 160, 960,
               960, 320, 1280]
    backbone:
      # [n, stride, c_in, c_out, [expand_ratio], channel_search, [op]]
      conv_stem: [1, 2, 3, 32, [], False, ['conv3x3']]
      stage_0: [1, 1, 32, 16, [1], False, ['ir_3x3']]
      stage1: [2, 2, 16, 24, [6], False, ['ir_3x3']]
      stage2: [3, 2, 24, 32, [6], False, ['ir_3x3']]
      stage3: [4, 2, 32, 64, [6], False, ['ir_3x3']]
      stage4: [3, 1, 64, 96, [6], False, ['ir_3x3']]
      stage5: [3, 2, 96, 160, [6], False, ['ir_3x3']]
      stage6: [1, 1, 160, 320, [6], False, ['ir_3x3']]
      conv_out: [1, 1, 320, 1280, [], False, ['conv2d']]
      final_pooling: True
    head:
      linear1:
        dim_in: 1280
        dim_out: 1000
    loss_type: 's-softmax'
    width_multiplier: 1.5

retrain:
    flag: True
    strategy:
        max_iter: 752000  #338000(300 epoch)   1252000(500 epoch)  752000 for sample
        max_iter_keeptrain: 50060 #20*2503
        optimizer:
            type: 'SGD'
            lr: 0.1  # in proxyless: 0.0125 per gpu
            weight_decay: 0.00005
            momentum: 0.9
            nesterov: True
        resume: False
        lr_scheduler:
            #base_lr: 0.12
            lr_steps: [50080, 100160, 125200]
            lr_mults: [0.1, 0.1, 0.1]
            warmup_steps: 12515  # warmup epochs: 5 steps=5*2503=12515
            warmup_strategy: 'gradual'
            warmup_lr: 0.2
            decay_stg: 'cosine'
            # final lr in cosine strategy
            alpha: 0.00001
            # how many iterations it takes to decay lr to 'alpha'
            decay_step: 732000
        bin_search: True
        bin_config:
            min_bin: 2
            max_bin: 64
            P_train: 0.5
            skip_list: [[0, 1], [3, 4], [5, 8], [6, 7], [9, 10], [11, 14, 17],
                        [12,13], [15, 16], [18, 19], [20, 23, 26, 29], [21, 22],
                        [24, 25], [27, 28], [30, 31], [32, 35, 38], [33,34], [36, 37],
                        [39, 40], [41, 44, 47], [42, 43], [45,46], [48, 49]]    # mobilenetv2

        task_type: 'imagenet'
        snapshot_freq: 2000
        print_freq: 20
        basic_path: '../generalNAS_exp/imagenet_mbnetv2_width_1.4_min2_max_64'
        save_path: '../generalNAS_exp/imagenet_mbnetv2_width_1.4_min2_max_64/retrain'

    data:
        workers: 8  # dataloader worker num
        task_type: 'imagenet'
        data_type: 'ssst'
        scatter_mode: False
        final_height: 224
        final_width: 224
        final_channel: 3
        augmentation:
            rand_resize:
                output_size: 224
                scale: [0.08, 1.0]
                ratio: [0.75, 1.3333333333]
            affine:
                mirror:
                    mirror_prob: 0.5
            # resize
            resize:
                output_size: [224, 224]
            # normalize
            normalize:
                normalize_type: 'mean_std'
                mean: [123.675, 116.28, 103.53]
                std: [58.395, 57.120, 57.375]
        imagenet:
            type: 'classification'
            task: 'imagenet'
            json_path: '/mnt/lustre/suxiu/dataset/imagenet/imagenet.json'
            prefix: '/mnt/lustre/share/images/train'
            batch_size: 64 # for single gpu

sample:
    flag: True
    epoch: 1  #sample epoch
    flops_constrant: 0.691  #207000000, changed to percent
    sampler:
        type: 'evolution'
        Deduce_P: 0.015  # init_sample = every epoch flops ^0.5 - Deduce_P
        kwargs:
            log_path: '../generalNAS_exp/imagenet_greedynas_nopool_search'
            Sample_test_num: None
            cal_time: 2 #cal time of cifar10 or imagenet
            search_num: 10 #10
            sample_num: 1
            pop_size: 40 #50
            n_gens: 20 #20
            extension_loc: [[0, 1], [3, 4], [5, 8], [6, 7], [9, 10], [11, 14, 17],
                            [12,13], [15, 16], [18, 19], [20, 23, 26, 29], [21, 22],
                            [24, 25], [27, 28], [30, 31], [32, 35, 38], [33,34], [36, 37],
                            [39, 40], [41, 44, 47], [42, 43], [45,46], [48, 49]]

    strategy:
        task_type: 'imagenet-test'
        snapshot_freq: 2000
        print_freq: 20
        use_basic_path: False
        save_path: '../generalNAS_exp/imagenet_mbnetv2_retrain_total/epoch0' #meiyongï¼Ÿ  '../generalNAS_exp/imagenet_greedynas_nopool_search'
        load_name: 'iter_752000_ckpt.pth.tar' #'iter_70000_ckpt.pth.tar'  #epoch == 1 and load_name == None for save_path and load_name useful

    data:
        workers: 8  # dataloader worker num
        task_type: 'imagenet-test'
        data_type: 'ssst'
        scatter_mode: False
        final_height: 224
        final_width: 224
        final_channel: 3
        augmentation:
            # resize
            resize:
                output_size: 256
            center_crop:
                output_size: 224
            # normalize
            normalize:
                normalize_type: 'mean_std'
                mean: [123.675, 116.28, 103.53]
                std: [58.395, 57.120, 57.375]
        imagenet:
            type: 'classification'
            task: 'imagenet'
            json_path: '/mnt/lustre/suxiu/dataset/imagenet/imagenet_val.json'
            prefix: '/mnt/lustre/share/images/val'
            batch_size: 128 # for single gpu

final_train:
    flag: True
    train_mode: 'retrain'  #it can be retrain or finetune
    # FLOPs: 333,598,784 FLOPs
    strategy:
      retrain:
        max_iter: 752000
        optimizer:
            type: 'SGD'
            lr: 0.1  # in proxyless: 0.0125 per gpu
            weight_decay: 0.00005
            momentum: 0.9
            nesterov: True
        resume: False
        lr_scheduler:
            #base_lr: 0.12
            lr_steps: [50080, 100160, 125200]
            lr_mults: [0.1, 0.1, 0.1]
            warmup_steps: 12515  # warmup epochs: 5 steps=5*2503=12515
            warmup_strategy: 'gradual'
            warmup_lr: 0.2
            decay_stg: 'cosine'
            # final lr in cosine strategy
            alpha: 0.00001
            # how many iterations it takes to decay lr to 'alpha'
            decay_step: 730000
        task_type: 'imagenet'
        snapshot_freq: 2000
        print_freq: 20
        use_basic_path: True
        save_path: '../generalNAS_exp/imagenet_mbnetv2_retrain/final_train'

      finetune:
        max_iter: 752000
        optimizer:
          type: 'SGD'
          lr: 0.1  # in proxyless: 0.0125 per gpu
          weight_decay: 0.00005
          momentum: 0.9
          nesterov: True
        resume: False
        lr_scheduler:
          #base_lr: 0.12
          lr_steps: [50080, 100160, 125200]
          lr_mults: [0.1, 0.1, 0.1]
          warmup_steps: 12515  # warmup epochs: 5 steps=5*2503=12515
          warmup_strategy: 'gradual'
          warmup_lr: 0.2
          decay_stg: 'cosine'
          # final lr in cosine strategy
          alpha: 0.00001
          # how many iterations it takes to decay lr to 'alpha'
        decay_step: 730000
        task_type: 'imagenet'
        snapshot_freq: 2000
        print_freq: 20
        use_basic_path: True
        save_path: '../generalNAS_exp/imagenet_mbnetv2_retrain/final_train'

    data:
        workers: 8  # dataloader worker num
        task_type: 'imagenet'
        data_type: 'ssst'
        scatter_mode: False
        final_height: 224
        final_width: 224
        final_channel: 3
        augmentation:
            rand_resize:
                output_size: 224
                scale: [0.08, 1.0]
                ratio: [0.75, 1.33]
            affine:
                mirror:
                    mirror_prob: 0.5
            # resize
            resize:
                output_size: [224, 224]
            # normalize
            normalize:
                normalize_type: 'mean_std'
                mean: [123.675, 116.28, 103.53]
                std: [58.395, 57.120, 57.375]
        imagenet:
            type: 'classification'
            task: 'imagenet'
            json_path: '/mnt/lustre/suxiu/dataset/imagenet/imagenet.json'
            prefix: '/mnt/lustre/share/images/train'
            batch_size: 64 # for single gpu



test:
    flag: True
    strategy:
        task_type: 'imagenet-test'
        snapshot_freq: 2000
        print_freq: 20
        use_basic_path: True
        save_path: '../generalNAS_exp/imagenet_mbnetv2_retrain/final_train/checkpoint'
        load_iter: 750000
        start: 742000
        end: 752000
        strip: 200

    data:
        workers: 8  # dataloader worker num
        task_type: 'imagenet-test'
        data_type: 'ssst'
        scatter_mode: False
        final_height: 224
        final_width: 224
        final_channel: 3
        augmentation:
            resize:
                output_size: 256
            center_crop:
                output_size: 224
            normalize:
                normalize_type: 'mean_std'
                mean: [123.675, 116.28, 103.53]
                std: [58.395, 57.120, 57.375]
        imagenet:
            type: 'classification'
            task: 'imagenet'
            json_path: '/mnt/lustre/suxiu/dataset/imagenet/imagenet_val.json'
            prefix: '/mnt/lustre/share/images/val'
            batch_size: 32 # for single gpu

